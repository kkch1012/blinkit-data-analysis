"""
MongoDB ìŠ¤í‚¤ë§ˆ ì„¤ê³„ ë° ì›¹ ì„œë¹„ìŠ¤ìš© ë°ì´í„° ëª¨ë¸
"""

from pymongo import MongoClient
from datetime import datetime
from typing import Dict, List, Optional
import json


class MongoDBService:
    """MongoDB ê¸°ë°˜ ë°ì´í„° ì„œë¹„ìŠ¤"""
    
    def __init__(self, connection_string: str = "mongodb://localhost:27017/", db_name: str = "blinkit_analytics"):
        """
        Args:
            connection_string: MongoDB ì—°ê²° ë¬¸ìì—´
            db_name: ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
        """
        self.client = MongoClient(connection_string)
        self.db = self.client[db_name]
        
        # ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        self.files = self.db['files']  # íŒŒì¼ ë©”íƒ€ë°ì´í„°
        self.csv_contents = self.db['csv_contents']  # ì‹¤ì œ CSV ë°ì´í„°
        self.analysis_results = self.db['analysis_results']  # ë¶„ì„ ê²°ê³¼
        self.feature_weights = self.db['feature_weights']  # í”¼ì²˜ ê°€ì¤‘ì¹˜
        self.user_suggestions = self.db['user_suggestions']  # ì‚¬ìš©ì ì œì•ˆ
    
    def upload_csv(self, user_id: str, file_path: str, 
                   file_name: str, file_size: int) -> Dict:
        """
        CSV íŒŒì¼ ì—…ë¡œë“œ ë° íŒŒì‹±
        
        Args:
            user_id: ì‚¬ìš©ì ID
            file_path: íŒŒì¼ ê²½ë¡œ
            file_name: íŒŒì¼ëª…
            file_size: íŒŒì¼ í¬ê¸°
        
        Returns:
            {
                'file_id': str,
                'columns': List[str],
                'row_count': int,
                'suggestions': List[str]
            }
        """
        import pandas as pd
        
        # CSV ì½ê¸°
        df = pd.read_csv(file_path)
        
        # íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥
        file_doc = {
            'file_id': f"file_{datetime.now().strftime('%Y%m%d%H%M%S')}_{user_id}",
            'user_id': user_id,
            'file_name': file_name,
            'file_size': file_size,
            'columns': df.columns.tolist(),
            'column_types': {col: str(df[col].dtype) for col in df.columns},
            'row_count': len(df),
            'uploaded_at': datetime.now(),
            'status': 'uploaded'
        }
        
        file_id = self.files.insert_one(file_doc).inserted_id
        file_doc['_id'] = file_id
        
        # CSV ë°ì´í„° ëŒ€ëŸ‰ ì‚½ì…
        csv_data = df.to_dict('records')
        csv_docs = []
        
        for idx, row in enumerate(csv_data):
            doc = {
                'file_id': file_doc['file_id'],
                'user_id': user_id,
                'row_index': idx,
                'data': row,
                'created_at': datetime.now()
            }
            csv_docs.append(doc)
            
            # ë°°ì¹˜ ì‚½ì… (1000ê°œì”©)
            if len(csv_docs) >= 1000:
                self.csv_contents.insert_many(csv_docs)
                csv_docs = []
        
        # ë‚¨ì€ ë°ì´í„° ì‚½ì…
        if csv_docs:
            self.csv_contents.insert_many(csv_docs)
        
        # ìë™ ë¶„ì„ ë° ì œì•ˆ ìƒì„±
        suggestions = self.generate_suggestions(file_doc)
        
        return {
            'file_id': file_doc['file_id'],
            'columns': df.columns.tolist(),
            'row_count': len(df),
            'suggestions': suggestions
        }
    
    def generate_suggestions(self, file_doc: Dict) -> List[str]:
        """
        íŒŒì¼ ì»¬ëŸ¼ì„ ë³´ê³  ìë™ ì œì•ˆ ìƒì„±
        
        Returns:
            ì œì•ˆ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        """
        suggestions = []
        columns = file_doc['columns']
        column_types = file_doc['column_types']
        
        # ê¸ˆì•¡/ìˆ˜ëŸ‰ ì»¬ëŸ¼ ê°ì§€
        amount_keywords = ['ê¸ˆì•¡', 'amount', 'price', 'ê°€ê²©', 'ë§¤ì¶œ', 'revenue']
        quantity_keywords = ['ìˆ˜ëŸ‰', 'quantity', 'qty', 'íŒë§¤ëŸ‰', 'sales']
        date_keywords = ['ë‚ ì§œ', 'date', 'ì¼ì', 'time', 'ì‹œê°„']
        
        has_amount = any(kw in col.lower() for col in columns for kw in amount_keywords)
        has_quantity = any(kw in col.lower() for col in columns for kw in quantity_keywords)
        has_date = any(kw in col.lower() for col in columns for kw in date_keywords)
        
        if has_amount:
            suggestions.append("ğŸ’° 'ê¸ˆì•¡' ì»¬ëŸ¼ì´ ìˆë„¤ìš”! í•©ê³„/í‰ê· ì„ êµ¬í•´ë“œë¦´ê¹Œìš”?")
        
        if has_quantity:
            suggestions.append("ğŸ“¦ 'ìˆ˜ëŸ‰' ì»¬ëŸ¼ì´ ìˆë„¤ìš”! ì´ íŒë§¤ëŸ‰ì„ ê³„ì‚°í•´ë“œë¦´ê¹Œìš”?")
        
        if has_date:
            suggestions.append("ğŸ“… ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆë„¤ìš”! ì‹œê³„ì—´ ë¶„ì„ì„ ì§„í–‰í• ê¹Œìš”?")
            suggestions.append("ğŸ“ˆ ì£¼ê°„/ì›”ê°„ íŠ¸ë Œë“œ ë¶„ì„ì„ í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
        
        # ë²”ì£¼í˜• ì»¬ëŸ¼ ê°ì§€
        categorical_cols = [col for col, dtype in column_types.items() 
                           if dtype == 'object' or col in ['ìƒí’ˆëª…', 'ì¹´í…Œê³ ë¦¬', 'ì§€ì—­']]
        if categorical_cols:
            suggestions.append(f"ğŸ·ï¸ ë²”ì£¼í˜• ì»¬ëŸ¼({', '.join(categorical_cols[:3])})ì´ ìˆë„¤ìš”! ê·¸ë£¹ë³„ ì§‘ê³„ë¥¼ í• ê¹Œìš”?")
        
        # ì œì•ˆ ì €ì¥
        suggestion_doc = {
            'file_id': file_doc['file_id'],
            'user_id': file_doc['user_id'],
            'suggestions': suggestions,
            'created_at': datetime.now()
        }
        self.user_suggestions.insert_one(suggestion_doc)
        
        return suggestions
    
    def save_analysis_result(self, file_id: str, user_id: str,
                           analysis_type: str, result: Dict) -> str:
        """
        ë¶„ì„ ê²°ê³¼ ì €ì¥
        
        Args:
            file_id: íŒŒì¼ ID
            user_id: ì‚¬ìš©ì ID
            analysis_type: ë¶„ì„ ìœ í˜• ('correlation', 'model', 'trend' ë“±)
            result: ë¶„ì„ ê²°ê³¼
        
        Returns:
            analysis_id
        """
        analysis_doc = {
            'analysis_id': f"analysis_{datetime.now().strftime('%Y%m%d%H%M%S')}",
            'file_id': file_id,
            'user_id': user_id,
            'analysis_type': analysis_type,
            'result': result,
            'created_at': datetime.now()
        }
        
        analysis_id = self.analysis_results.insert_one(analysis_doc).inserted_id
        return str(analysis_id)
    
    def save_feature_weights(self, file_id: str, user_id: str,
                            weights: Dict[str, float],
                            model_metrics: Dict) -> str:
        """
        í”¼ì²˜ ê°€ì¤‘ì¹˜ ì €ì¥
        
        Args:
            file_id: íŒŒì¼ ID
            user_id: ì‚¬ìš©ì ID
            weights: {feature_name: weight}
            model_metrics: ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ
        
        Returns:
            weight_id
        """
        weight_doc = {
            'weight_id': f"weight_{datetime.now().strftime('%Y%m%d%H%M%S')}",
            'file_id': file_id,
            'user_id': user_id,
            'weights': weights,
            'model_metrics': model_metrics,
            'created_at': datetime.now()
        }
        
        weight_id = self.feature_weights.insert_one(weight_doc).inserted_id
        return str(weight_id)
    
    def get_file_data(self, file_id: str, limit: int = 100) -> List[Dict]:
        """
        íŒŒì¼ ë°ì´í„° ì¡°íšŒ
        
        Args:
            file_id: íŒŒì¼ ID
            limit: ì¡°íšŒí•  í–‰ ìˆ˜
        
        Returns:
            ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        cursor = self.csv_contents.find(
            {'file_id': file_id}
        ).sort('row_index', 1).limit(limit)
        
        return [doc['data'] for doc in cursor]
    
    def get_user_files(self, user_id: str) -> List[Dict]:
        """
        ì‚¬ìš©ìì˜ ëª¨ë“  íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        """
        files = list(self.files.find({'user_id': user_id}).sort('uploaded_at', -1))
        return files
    
    def get_suggestions(self, file_id: str) -> List[str]:
        """
        íŒŒì¼ì— ëŒ€í•œ ì œì•ˆ ì¡°íšŒ
        """
        suggestion = self.user_suggestions.find_one({'file_id': file_id})
        return suggestion['suggestions'] if suggestion else []


# MongoDB ìŠ¤í‚¤ë§ˆ ë¬¸ì„œí™”
SCHEMA_DOCUMENTATION = """
# MongoDB ìŠ¤í‚¤ë§ˆ ì„¤ê³„

## 1. files ì»¬ë ‰ì…˜ (íŒŒì¼ ë©”íƒ€ë°ì´í„°)
```json
{
  "_id": ObjectId,
  "file_id": "file_20241229120000_user123",
  "user_id": "user123",
  "file_name": "blinkit_data.csv",
  "file_size": 1024000,
  "columns": ["ì£¼ë¬¸ë‚ ì§œ", "ìƒí’ˆëª…", "ìˆ˜ëŸ‰", "ê¸ˆì•¡"],
  "column_types": {
    "ì£¼ë¬¸ë‚ ì§œ": "object",
    "ìƒí’ˆëª…": "object",
    "ìˆ˜ëŸ‰": "int64",
    "ê¸ˆì•¡": "float64"
  },
  "row_count": 5000,
  "uploaded_at": ISODate("2024-12-29T12:00:00Z"),
  "status": "uploaded"
}
```

## 2. csv_contents ì»¬ë ‰ì…˜ (ì‹¤ì œ ë°ì´í„°)
```json
{
  "_id": ObjectId,
  "file_id": "file_20241229120000_user123",
  "user_id": "user123",
  "row_index": 0,
  "data": {
    "ì£¼ë¬¸ë‚ ì§œ": "2024-07-17",
    "ìƒí’ˆëª…": "Pet Treats",
    "ìˆ˜ëŸ‰": 3,
    "ê¸ˆì•¡": 1551.09
  },
  "created_at": ISODate("2024-12-29T12:00:00Z")
}
```

## 3. analysis_results ì»¬ë ‰ì…˜ (ë¶„ì„ ê²°ê³¼)
```json
{
  "_id": ObjectId,
  "analysis_id": "analysis_20241229120000",
  "file_id": "file_20241229120000_user123",
  "user_id": "user123",
  "analysis_type": "correlation",
  "result": {
    "correlation_matrix": {...},
    "top_correlations": [...]
  },
  "created_at": ISODate("2024-12-29T12:00:00Z")
}
```

## 4. feature_weights ì»¬ë ‰ì…˜ (í”¼ì²˜ ê°€ì¤‘ì¹˜)
```json
{
  "_id": ObjectId,
  "weight_id": "weight_20241229120000",
  "file_id": "file_20241229120000_user123",
  "user_id": "user123",
  "weights": {
    "ìˆ˜ëŸ‰_lag_1": 0.25,
    "temp_max": 0.15,
    "spend": 0.20,
    ...
  },
  "model_metrics": {
    "mae": 1.23,
    "r2": 0.65,
    "accuracy": 72.5
  },
  "created_at": ISODate("2024-12-29T12:00:00Z")
}
```

## 5. user_suggestions ì»¬ë ‰ì…˜ (ì‚¬ìš©ì ì œì•ˆ)
```json
{
  "_id": ObjectId,
  "file_id": "file_20241229120000_user123",
  "user_id": "user123",
  "suggestions": [
    "ğŸ’° 'ê¸ˆì•¡' ì»¬ëŸ¼ì´ ìˆë„¤ìš”! í•©ê³„/í‰ê· ì„ êµ¬í•´ë“œë¦´ê¹Œìš”?",
    "ğŸ“¦ 'ìˆ˜ëŸ‰' ì»¬ëŸ¼ì´ ìˆë„¤ìš”! ì´ íŒë§¤ëŸ‰ì„ ê³„ì‚°í•´ë“œë¦´ê¹Œìš”?",
    "ğŸ“… ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆë„¤ìš”! ì‹œê³„ì—´ ë¶„ì„ì„ ì§„í–‰í• ê¹Œìš”?"
  ],
  "created_at": ISODate("2024-12-29T12:00:00Z")
}
```

## ì¸ë±ìŠ¤ ì„¤ê³„
```javascript
// files ì»¬ë ‰ì…˜
db.files.createIndex({ "user_id": 1, "uploaded_at": -1 })
db.files.createIndex({ "file_id": 1 })

// csv_contents ì»¬ë ‰ì…˜
db.csv_contents.createIndex({ "file_id": 1, "row_index": 1 })
db.csv_contents.createIndex({ "user_id": 1 })

// analysis_results ì»¬ë ‰ì…˜
db.analysis_results.createIndex({ "file_id": 1, "analysis_type": 1 })
db.analysis_results.createIndex({ "user_id": 1, "created_at": -1 })

// feature_weights ì»¬ë ‰ì…˜
db.feature_weights.createIndex({ "file_id": 1 })
db.feature_weights.createIndex({ "user_id": 1 })
```
"""


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # MongoDB ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    mongo_service = MongoDBService(
        connection_string="mongodb://localhost:27017/",
        db_name="blinkit_analytics"
    )
    
    # CSV ì—…ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜
    result = mongo_service.upload_csv(
        user_id="user123",
        file_path="data/blinkit_with_weather.csv",
        file_name="blinkit_with_weather.csv",
        file_size=1024000
    )
    
    print("="*60)
    print("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")
    print("="*60)
    print(f"File ID: {result['file_id']}")
    print(f"ì»¬ëŸ¼ ìˆ˜: {len(result['columns'])}")
    print(f"í–‰ ìˆ˜: {result['row_count']}")
    print(f"\nğŸ’¡ ìë™ ì œì•ˆ:")
    for suggestion in result['suggestions']:
        print(f"  - {suggestion}")
    
    # í”¼ì²˜ ê°€ì¤‘ì¹˜ ì €ì¥ ì˜ˆì‹œ
    weights = {
        'ìˆ˜ëŸ‰_lag_1': 0.25,
        'temp_max': 0.15,
        'spend': 0.20,
        'rainfall': 0.10
    }
    
    metrics = {
        'mae': 1.23,
        'r2': 0.65,
        'accuracy': 72.5
    }
    
    weight_id = mongo_service.save_feature_weights(
        file_id=result['file_id'],
        user_id="user123",
        weights=weights,
        model_metrics=metrics
    )
    
    print(f"\nğŸ’¾ ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ: {weight_id}")

