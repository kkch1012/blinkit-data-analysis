"""
ìë™í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° ê°€ì¤‘ì¹˜ ìƒì„± íŒŒì´í”„ë¼ì¸
- ì–´ë–¤ CSVë¥¼ ë„£ì–´ë„ ìë™ìœ¼ë¡œ ì»¬ëŸ¼ ê°ì§€, í”¼ì²˜ ìƒì„±, ê°€ì¤‘ì¹˜ ê³„ì‚°
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')


class AutoFeaturePipeline:
    """ìë™í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, target_column: Optional[str] = None):
        """
        Args:
            target_column: ì˜ˆì¸¡í•  íƒ€ê²Ÿ ì»¬ëŸ¼ëª… (Noneì´ë©´ ìë™ ê°ì§€)
        """
        self.target_column = target_column
        self.encoders = {}
        self.scaler = MinMaxScaler()
        self.feature_config = {}
        self.correlation_weights = {}
        
    def detect_columns(self, df: pd.DataFrame) -> Dict[str, List[str]]:
        """
        ì»¬ëŸ¼ ìë™ ê°ì§€ ë° ë¶„ë¥˜
        
        Returns:
            {
                'date_columns': [...],
                'categorical_columns': [...],
                'numeric_columns': [...],
                'target_candidates': [...]
            }
        """
        result = {
            'date_columns': [],
            'categorical_columns': [],
            'numeric_columns': [],
            'target_candidates': []
        }
        
        for col in df.columns:
            # ë‚ ì§œ ì»¬ëŸ¼ ê°ì§€
            if df[col].dtype == 'object':
                try:
                    pd.to_datetime(df[col].head(10))
                    result['date_columns'].append(col)
                    continue
                except:
                    pass
            
            # ë²”ì£¼í˜• vs ìˆ˜ì¹˜í˜•
            if df[col].dtype == 'object' or df[col].nunique() < df.shape[0] * 0.1:
                result['categorical_columns'].append(col)
            else:
                result['numeric_columns'].append(col)
                
                # íƒ€ê²Ÿ í›„ë³´ (ìˆ˜ì¹˜í˜• ì¤‘ì—ì„œ)
                if col.lower() in ['ìˆ˜ëŸ‰', 'quantity', 'qty', 'sales', 'íŒë§¤ëŸ‰', 'amount', 'ê¸ˆì•¡', 'price']:
                    result['target_candidates'].append(col)
        
        return result
    
    def auto_feature_engineering(self, df: pd.DataFrame, 
                                 group_by: Optional[List[str]] = None) -> pd.DataFrame:
        """
        ìë™ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
        
        Args:
            df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„
            group_by: ê·¸ë£¹í™”í•  ì»¬ëŸ¼ (ì˜ˆ: ['ìƒí’ˆëª…', 'ì§€ì—­'])
        
        Returns:
            í”¼ì²˜ê°€ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        """
        df = df.copy()
        column_info = self.detect_columns(df)
        
        # 1. ë‚ ì§œ í”¼ì²˜ ìƒì„±
        for date_col in column_info['date_columns']:
            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
            df[f'{date_col}_month'] = df[date_col].dt.month
            df[f'{date_col}_day_of_week'] = df[date_col].dt.dayofweek
            df[f'{date_col}_is_weekend'] = (df[date_col].dt.dayofweek >= 5).astype(int)
            df[f'{date_col}_day'] = df[date_col].dt.day
        
        # 2. íƒ€ê²Ÿ ì»¬ëŸ¼ ìë™ ê°ì§€
        if not self.target_column:
            if column_info['target_candidates']:
                self.target_column = column_info['target_candidates'][0]
            elif column_info['numeric_columns']:
                # ê°€ì¥ ë§ˆì§€ë§‰ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì„ íƒ€ê²Ÿìœ¼ë¡œ
                self.target_column = column_info['numeric_columns'][-1]
            else:
                raise ValueError("íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"âœ… íƒ€ê²Ÿ ì»¬ëŸ¼: {self.target_column}")
        
        # 3. ì‹œê³„ì—´ í”¼ì²˜ ìƒì„± (group_by ê¸°ì¤€)
        if group_by and self.target_column:
            for lag in [1, 7, 14]:
                df[f'{self.target_column}_lag_{lag}'] = df.groupby(group_by)[self.target_column].shift(lag)
            
            # ì´ë™í‰ê· 
            for window in [3, 7]:
                df[f'{self.target_column}_MA{window}'] = df.groupby(group_by)[self.target_column].transform(
                    lambda x: x.rolling(window=window, min_periods=1).mean()
                ).shift(1)
            
            # ë³€í™”ëŸ‰
            if f'{self.target_column}_lag_1' in df.columns:
                df[f'{self.target_column}_change'] = df[self.target_column] - df[f'{self.target_column}_lag_1']
        
        # 4. ë²”ì£¼í˜• ì¸ì½”ë”©
        for cat_col in column_info['categorical_columns']:
            if cat_col not in df.columns:
                continue
            le = LabelEncoder()
            df[f'{cat_col}_encoded'] = le.fit_transform(df[cat_col].astype(str))
            self.encoders[cat_col] = le
        
        # 5. ìˆ˜ì¹˜í˜• ì •ê·œí™” (ì„ íƒì )
        numeric_cols = [col for col in column_info['numeric_columns'] 
                       if col != self.target_column and col in df.columns]
        if numeric_cols:
            df[numeric_cols] = self.scaler.fit_transform(df[numeric_cols])
        
        return df
    
    def calculate_correlation_weights(self, df: pd.DataFrame, 
                                     target_col: str,
                                     feature_cols: List[str]) -> Dict[str, float]:
        """
        ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        
        Returns:
            {feature_name: correlation_weight}
        """
        weights = {}
        
        for feat in feature_cols:
            if feat in df.columns:
                corr = df[[target_col, feat]].corr().iloc[0, 1]
                if not np.isnan(corr):
                    weights[feat] = abs(corr)  # ì ˆëŒ“ê°’ ì‚¬ìš©
        
        # ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
        total = sum(weights.values())
        if total > 0:
            weights = {k: v/total for k, v in weights.items()}
        
        return weights
    
    def create_weighted_score(self, df: pd.DataFrame, 
                             weights: Dict[str, float]) -> pd.Series:
        """
        ê°€ì¤‘í•© ì ìˆ˜ ìƒì„±
        """
        weighted_score = pd.Series(0.0, index=df.index)
        
        for feat, weight in weights.items():
            if feat in df.columns:
                weighted_score += df[feat] * weight
        
        return weighted_score
    
    def train_model(self, df: pd.DataFrame, 
                   feature_columns: List[str],
                   target_column: str,
                   test_size: float = 0.2) -> Tuple[RandomForestRegressor, Dict]:
        """
        ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
        
        Returns:
            (model, metrics)
        """
        # ê²°ì¸¡ì¹˜ ì œê±°
        df_clean = df[feature_columns + [target_column]].dropna()
        
        if len(df_clean) < 50:
            raise ValueError(f"ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. (ìµœì†Œ 50ê°œ í•„ìš”, í˜„ì¬ {len(df_clean)}ê°œ)")
        
        X = df_clean[feature_columns]
        y = df_clean[target_column]
        
        # ì‹œê³„ì—´ ë°ì´í„°ì´ë¯€ë¡œ ìˆœì°¨ì  ë¶„í• 
        split = int(len(X) * (1 - test_size))
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        # ëª¨ë¸ í•™ìŠµ
        model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
        model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred = model.predict(X_test)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        accuracy = (1 - mae / y_test.mean()) * 100 if y_test.mean() != 0 else 0
        
        metrics = {
            'mae': float(mae),
            'r2': float(r2),
            'accuracy': float(accuracy),
            'train_size': len(X_train),
            'test_size': len(X_test)
        }
        
        return model, metrics
    
    def process_csv(self, csv_path: str, 
                   group_by: Optional[List[str]] = None,
                   save_config: bool = True) -> Dict:
        """
        CSV íŒŒì¼ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        
        Returns:
            {
                'data': processed_df,
                'model': trained_model,
                'metrics': metrics,
                'weights': correlation_weights,
                'config': feature_config
            }
        """
        print(f"ğŸ“ CSV íŒŒì¼ ë¡œë“œ: {csv_path}")
        df = pd.read_csv(csv_path)
        print(f"   Shape: {df.shape}")
        
        # ì»¬ëŸ¼ ê°ì§€
        column_info = self.detect_columns(df)
        print(f"\nğŸ“Š ì»¬ëŸ¼ ë¶„ë¥˜:")
        print(f"   - ë‚ ì§œ: {column_info['date_columns']}")
        print(f"   - ë²”ì£¼í˜•: {column_info['categorical_columns']}")
        print(f"   - ìˆ˜ì¹˜í˜•: {column_info['numeric_columns']}")
        print(f"   - íƒ€ê²Ÿ í›„ë³´: {column_info['target_candidates']}")
        
        # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
        print(f"\nğŸ”§ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¤‘...")
        df_processed = self.auto_feature_engineering(df, group_by=group_by)
        print(f"   ì²˜ë¦¬ í›„ Shape: {df_processed.shape}")
        
        # í”¼ì²˜ ì»¬ëŸ¼ ì„ íƒ
        feature_cols = []
        for col in df_processed.columns:
            if col != self.target_column and not col.endswith('_encoded'):
                if df_processed[col].dtype in ['int64', 'float64']:
                    feature_cols.append(col)
        
        # ì¸ì½”ë”©ëœ ì»¬ëŸ¼ë„ ì¶”ê°€
        encoded_cols = [col for col in df_processed.columns if col.endswith('_encoded')]
        feature_cols.extend(encoded_cols)
        
        print(f"\nğŸ“‹ ì„ íƒëœ í”¼ì²˜ ({len(feature_cols)}ê°œ):")
        print(f"   {feature_cols[:10]}...")
        
        # ìƒê´€ê³„ìˆ˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        print(f"\nâš–ï¸ ê°€ì¤‘ì¹˜ ê³„ì‚° ì¤‘...")
        correlation_weights = self.calculate_correlation_weights(
            df_processed, self.target_column, feature_cols
        )
        self.correlation_weights = correlation_weights
        
        # ê°€ì¤‘í•© ì ìˆ˜ ìƒì„±
        df_processed['weighted_score'] = self.create_weighted_score(
            df_processed, correlation_weights
        )
        feature_cols.append('weighted_score')
        
        # ëª¨ë¸ í•™ìŠµ
        print(f"\nğŸ¤– ëª¨ë¸ í•™ìŠµ ì¤‘...")
        model, metrics = self.train_model(
            df_processed, feature_cols, self.target_column
        )
        
        print(f"\nâœ… ì™„ë£Œ!")
        print(f"   MAE: {metrics['mae']:.4f}")
        print(f"   RÂ²: {metrics['r2']:.4f}")
        print(f"   ì •í™•ë„: {metrics['accuracy']:.2f}%")
        
        # ì„¤ì • ì €ì¥
        self.feature_config = {
            'target_column': self.target_column,
            'feature_columns': feature_cols,
            'group_by': group_by,
            'column_info': column_info,
            'correlation_weights': correlation_weights,
            'metrics': metrics,
            'created_at': datetime.now().isoformat()
        }
        
        if save_config:
            config_path = csv_path.replace('.csv', '_config.json')
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(self.feature_config, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ì„¤ì • ì €ì¥: {config_path}")
        
        return {
            'data': df_processed,
            'model': model,
            'metrics': metrics,
            'weights': correlation_weights,
            'config': self.feature_config
        }


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì˜ˆì‹œ 1: ê¸°ë³¸ ì‚¬ìš©
    pipeline = AutoFeaturePipeline()
    
    # CSV ì²˜ë¦¬
    result = pipeline.process_csv(
        csv_path='data/blinkit_with_weather.csv',
        group_by=['ìƒí’ˆëª…', 'ì§€ì—­']  # ì§€ì—­ë³„, ìƒí’ˆë³„ ê·¸ë£¹í™”
    )
    
    print("\n" + "="*60)
    print("ğŸ“Š ê²°ê³¼ ìš”ì•½")
    print("="*60)
    print(f"ì²˜ë¦¬ëœ ë°ì´í„°: {result['data'].shape}")
    print(f"ëª¨ë¸ ì •í™•ë„: {result['metrics']['accuracy']:.2f}%")
    print(f"\nìƒìœ„ 5ê°œ í”¼ì²˜ ê°€ì¤‘ì¹˜:")
    sorted_weights = sorted(result['weights'].items(), key=lambda x: x[1], reverse=True)
    for feat, weight in sorted_weights[:5]:
        print(f"  {feat}: {weight:.4f}")

